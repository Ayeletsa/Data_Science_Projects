{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# wis_dnn_challenge.py\n",
    "# Description: This is a template file for the WIS DNN challenge submission.\n",
    "# Important: The only thing you should not change is the signature of the class (Predictor) and its predict function.\n",
    "#            Anything else is for you to decide how to implement.\n",
    "#            We provide you with a very basic working version of this class.\n",
    "#\n",
    "# Author: <first name1>_<last name1> [<first name1>_<last name2>]\n",
    "#\n",
    "# Python 3.7\n",
    "####################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# The time series that you would get are such that the difference between two rows is 15 minutes.\n",
    "# This is a global number that we used to prepare the data, so you would need it for different purposes.\n",
    "DATA_RESOLUTION_MIN = 15\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    \"\"\"\n",
    "    This is where you should implement your predictor.\n",
    "    The testing script calls the 'predict' function with the glucose and meals test data which you will need in order to\n",
    "    build your features for prediction.\n",
    "    You should implement this function as you wish, just do not change the function's signature (name, parameters).\n",
    "    The other functions are here just as an example for you to have something to start with, you may implement whatever\n",
    "    you wish however you see fit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path2data):\n",
    "        \"\"\"\n",
    "        This constructor only gets the path to a folder where the training data frames are.\n",
    "        :param path2data: a folder with your training data.\n",
    "        \"\"\"\n",
    "        self.path2data = path2data\n",
    "        self.train_glucose = None\n",
    "        self.train_meals = None\n",
    "        self.nn = None\n",
    "\n",
    "    def predict(self, X_glucose, X_meals):\n",
    "        \"\"\"\n",
    "        You must not change the signature of this function!!!\n",
    "        You are given two data frames: glucose values and meals.\n",
    "        For every timestamp (t) in X_glucose for which you have at least 12 hours (48 points) of past glucose and two\n",
    "        hours (8 points) of future glucose, predict the difference in glucose values for the next 8 time stamps\n",
    "        (t+15, t+30, ..., t+120).\n",
    "\n",
    "        :param X_glucose: A pandas data frame holding the glucose values in the format you trained on.\n",
    "        :param X_meals: A pandas data frame holding the meals data in the format you trained on.\n",
    "        :return: A numpy ndarray, sized (M x 8) holding your predictions for every valid row in X_glucose.\n",
    "                 M is the number of valid rows in X_glucose (number of time stamps for which you have at least 12 hours\n",
    "                 of past glucose values and 2 hours of future glucose values.\n",
    "                 Every row in your final ndarray should correspond to:\n",
    "                 (glucose[t+15min]-glucose[t], glucose[t+30min]-glucose[t], ..., glucose[t+120min]-glucose[t])\n",
    "        \"\"\"\n",
    "\n",
    "        # build features for set of (ID, timestamp)\n",
    "        ids = self.cgm_and_meals_id_time.reset_index()['id'].unique()        \n",
    "        x_all, y_all, id_all,x_GV= self.build_features(ids=ids)\n",
    "\n",
    "        #load nn:\n",
    "        model=load_nn_model(self)        \n",
    "       \n",
    "        # feed the network you trained\n",
    "        y_predict = model.predict([x_GV,x_all])\n",
    "       \n",
    "        #create data frame with idx:\n",
    "        y=pd.DataFrame(y_predict,index=id_all.ravel().astype(int))\n",
    "        y.index.names = ['id']\n",
    "       \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def define_nn_hypPars(self, BATCH_SIZE = 32, EVALUATION_INTERVAL = 10000, EPOCHS = 10, num_filters = [6,12,24,36,48,60,72,84,96],\n",
    "                          kernel_size = [3,3,3,3,3,3,3,3,3], LR = 0.1, n_LSTM = [48,16,16,16,16,16], n_dense = [16,16,16,16,16]):\n",
    "        \"\"\"\n",
    "        Define your neural network.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "       \n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.EVALUATION_INTERVAL = EVALUATION_INTERVAL\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.LR = LR\n",
    "        self.n_LSTM = n_LSTM\n",
    "        self.n_dense = n_dense\n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    def define_nn(self,GV_shape,MealsAndGV_shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define your neural network.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Get Hyper parameters\n",
    "        #Predictor.define_nn_hypPars(self)\n",
    "        \n",
    "        #### A split model - LSTM for GV, CNN for GV+Meals, FC after concatenation using functional API\n",
    "\n",
    "        ## define inputs for the model\n",
    "        GV_input = tf.keras.layers.Input(shape=GV_shape, name='GVinput')\n",
    "        MealsAndGV_input = tf.keras.layers.Input(shape=MealsAndGV_shape, name='MealsAndGVinput')\n",
    "\n",
    "        ## Build first branch - LSTM + FC for GV data\n",
    "        activation_GV = 'relu'\n",
    "\n",
    "        GV_lstm1 = tf.keras.layers.LSTM(self.n_LSTM[0],return_sequences=True)(GV_input)\n",
    "        GV_lstm2 = tf.keras.layers.LSTM(self.n_LSTM[1])(GV_lstm1)\n",
    "        GVoutput = tf.keras.layers.Dense(self.n_dense[0],activation=activation_GV,name='GVoutput')(GV_lstm2)\n",
    "        # GVoutput = tf.keras.layers.BatchNormalization()(GV_Dense)\n",
    "\n",
    "        ## Build second branch - CNN + FC for Meals and GV data\n",
    "        activation_MealsAndGV = 'relu'\n",
    "        MealsAndGV_conv1 = tf.keras.layers.Conv1D(self.num_filters[0],self.kernel_size[0], \n",
    "                                                  activation=activation_MealsAndGV)(MealsAndGV_input)\n",
    "        MealsAndGV_conv2 = tf.keras.layers.Conv1D(self.num_filters[1],self.kernel_size[1], \n",
    "                                                  activation=activation_MealsAndGV)(MealsAndGV_conv1)\n",
    "        MealsAndGV_conv3 = tf.keras.layers.Conv1D(self.num_filters[2],self.kernel_size[2], \n",
    "                                                  activation=activation_MealsAndGV)(MealsAndGV_conv2)\n",
    "        MealsAndGV_conv4 = tf.keras.layers.Conv1D(self.num_filters[3],self.kernel_size[3], \n",
    "                                                  activation=activation_MealsAndGV)(MealsAndGV_conv3)\n",
    "        # MealsAndGV_pool = tf.keras.layers.MaxPooling1D(2)(MealsAndGV_conv_third)\n",
    "        MealsAndGV_flat = tf.keras.layers.Flatten()(MealsAndGV_conv4)\n",
    "        MealsAndGV_output = tf.keras.layers.Dense(self.n_dense[1],activation=activation_MealsAndGV,name='Mealsoutput')(MealsAndGV_flat)\n",
    "        # MealsAndGV_output = tf.keras.layers.BatchNormalization()(MealsAndGV_conv_second)\n",
    "\n",
    "        ## Concatenate branches and generate output\n",
    "        activation_concat = 'relu'\n",
    "        combined = tf.keras.layers.concatenate([GVoutput, MealsAndGV_output])\n",
    "\n",
    "        combined_Dense1 = tf.keras.layers.Dense(self.n_dense[2],activation=activation_concat)(combined)\n",
    "        combined_output = tf.keras.layers.Dense(8,name='combined_output')(combined_Dense1)\n",
    "        \n",
    "        ## Build and compile the Final model\n",
    "        self.nn = tf.keras.Model(inputs=[GV_input,MealsAndGV_input], outputs=[combined_output]) \n",
    "        self.nn.compile(optimize='adam',loss='mae',learning_rate = self.LR)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def train_nn(self, X_train, y_train):\n",
    "\n",
    "        \"\"\"\n",
    "        Train your network using the training data.\n",
    "        :param X_train: A pandas data frame holding the features\n",
    "        :param y_train: A numpy ndarray, sized (M x 8) holding the values you need to predict.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ## Get Hyper parameters\n",
    "        #Predictor.define_nn_hypPars(self)\n",
    "\n",
    "        ## Split the input\n",
    "        X_train_GV = X_train[:,:,0:1]\n",
    "\n",
    "        ## Define callbacks to save data during training\n",
    "        checkpointer = ModelCheckpoint(filepath=self.path2data+'{epoch:02d}-'+str(self.BATCH_SIZE)+\n",
    "                           '-'+str(self.n_LSTM[0])+'.hdf5',\n",
    "                           save_best_only=False)\n",
    "\n",
    "        ## Train the network\n",
    "        self.nn.fit([X_train_GV,X_train],y_train,epochs=self.EPOCHS,\n",
    "                    batch_size=self.BATCH_SIZE,steps_per_epoch=self.EVALUATION_INTERVAL,\n",
    "                   callbacks=[checkpointer])\n",
    "\n",
    "        # save_nn_model(self)\n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def save_nn_model(self):\n",
    "        \"\"\"\n",
    "        Save your neural network after training.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_nn_model_for_train(self,epoch_str):\n",
    "        \"\"\"\n",
    "        Load your trained neural network.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.current_model = tf.keras.models.load_model(self.path2data+epoch_str+'-'+str(self.BATCH_SIZE)+\n",
    "                               '-'+str(self.n_LSTM[0])+'.hdf5')\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def load_nn_model(self):\n",
    "        \"\"\"\n",
    "        Load your trained neural network.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data_frame(path):\n",
    "        \"\"\"\n",
    "        Load a pandas data frame in the relevant format.\n",
    "        :param path: path to csv.\n",
    "        :return: the loaded data frame.\n",
    "        \"\"\"\n",
    "        return pd.read_csv(path, index_col=[0, 1], parse_dates=['Date'])\n",
    "\n",
    "    def load_raw_data(self,train=False):\n",
    "        \"\"\"\n",
    "        Loads raw data frames from csv files, and do some basic cleaning\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train_glucose = Predictor.load_data_frame(os.path.join(self.path2data, 'GlucoseValues.csv'))\n",
    "        self.train_meals = Predictor.load_data_frame(os.path.join(self.path2data, 'Meals.csv'))\n",
    "\n",
    "        # remove food_id and unit_id columns from Meals df\n",
    "        self.train_meals = self.train_meals.drop(['meal_type','unit_id','food_id'],axis=1)\n",
    "       \n",
    "       \n",
    "        if train:\n",
    "        # define  normaliation function, assumes an 'id' feature in dataframe!\n",
    "            def normalize_df (df):\n",
    "                mean_pd_by_id = df.groupby('id').mean()\n",
    "                std_pd_by_id = df.groupby('id').std()\n",
    "\n",
    "                df['Norm_GV'] = (df-mean_pd_by_id)/std_pd_by_id\n",
    "\n",
    "                return df\n",
    "\n",
    "            # normalize GV for every individual\n",
    "            self.train_glucose = normalize_df (self.train_glucose)\n",
    "\n",
    "            # remove outliers - thr std's above the mean\n",
    "            thr_std_outlier = 4\n",
    "            self.train_glucose = self.train_glucose[self.train_glucose['Norm_GV'].abs() < thr_std_outlier]\n",
    "\n",
    "            # re-normalize after removing outliers\n",
    "            self.train_glucose = normalize_df (self.train_glucose)\n",
    "           \n",
    "            #remove normalization:\n",
    "            self.train_glucose= self.train_glucose.drop(['Norm_GV'],axis=1)\n",
    "                 \n",
    "            # remove outliers of meals - thr std's above the mean\n",
    "            thr_std = 4\n",
    "            thr_outlier = self.train_meals.copy().replace(0,np.nan)\n",
    "            thr_outlier = thr_outlier.groupby('id').mean() + thr_std * thr_outlier.groupby('id').std()\n",
    "            self.train_meals = self.train_meals[self.train_meals - thr_outlier < 0]\n",
    "           \n",
    "                  \n",
    "\n",
    "        # resample meals using the timestamps from the glucose df\n",
    "        timeStamp = str(DATA_RESOLUTION_MIN)+'T'\n",
    "        self.train_meals = self.train_meals.groupby(pd.Grouper(level=0)).resample(timeStamp,level=-1).sum()\n",
    "\n",
    "        # Normalize after removing outliers\n",
    "        self.train_meals = self.train_meals/self.train_meals.groupby('id').max()\n",
    "\n",
    "        # resample glucose dataframe in timestamps identical to meals\n",
    "        self.train_glucose = self.train_glucose.groupby(pd.Grouper(level=0)).resample(timeStamp,level=-1).first()        \n",
    "       \n",
    "        # replace NAN values in Glucose with linear interpolation\n",
    "        self.train_glucose = self.train_glucose.interpolate()\n",
    "                \n",
    "        # now merge the glucose and meals dataframes on the same time stamps\n",
    "        self.cgm_and_meals_id_time = self.train_glucose.merge(self.train_meals,how='left',left_index=True,right_index=True)\n",
    "\n",
    "        # replace NAN values in meals with 0's\n",
    "        self.cgm_and_meals_id_time = self.cgm_and_meals_id_time.fillna(0)\n",
    "        self.X_glucose=self.cgm_and_meals_id_time.iloc[:,0:1]\n",
    "        self.X_meals=self.cgm_and_meals_id_time.iloc[:,1:]\n",
    "     \n",
    "        return self.X_glucose, self.X_meals\n",
    "   \n",
    "   \n",
    "    def build_features(self,ids, build_y=False, n_previous_time_points=48):\n",
    "        \"\"\"\n",
    "        Given glucose and meals data, build the features needed for prediction.\n",
    "        :param X_glucose: A pandas data frame holding the glucose values.\n",
    "        :param X_meals: A pandas data frame holding the meals data.\n",
    "        :param build_y: Whether to also extract the values needed for prediction.\n",
    "        :param n_previous_time_points:\n",
    "        :return: The features needed for your prediction, and optionally also the relevant y arrays for training.\n",
    "        \"\"\"\n",
    "        # function that create shifts:\n",
    "        def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "            data = []\n",
    "            labels = []\n",
    "            start_index=0\n",
    "            start_index = start_index + history_size\n",
    "            if end_index is None:\n",
    "                end_index = len(dataset) - target_size\n",
    "     \n",
    "\n",
    "            for i in range(start_index, end_index):\n",
    "                indices = range(i-history_size, i, step)\n",
    "                data.append(dataset[indices])\n",
    "\n",
    "                if single_step:\n",
    "                   labels.append(target[i+target_size])\n",
    "                else:\n",
    "                   labels.append(target[i:i+target_size]-target[i-1])\n",
    "\n",
    "            return np.array(data), np.array(labels)\n",
    "\n",
    "        # ,,,,,,,,,\n",
    "        past_history = n_previous_time_points\n",
    "        future_target = 8\n",
    "        STEP = 1\n",
    "\n",
    "   \n",
    "        x_all = np.empty([1,past_history,self.cgm_and_meals_id_time.shape[1]])\n",
    "        y_all = np.empty([1,future_target])\n",
    "        id_all = np.empty([1,1])\n",
    "\n",
    "        for id in ids:\n",
    "            multi_data = self.cgm_and_meals_id_time.loc[id]\n",
    "            end_index = len(multi_data)-future_target\n",
    "            x_single, y_single = multivariate_data(multi_data.values, multi_data.GlucoseValue.values, 0,\n",
    "                                                   len(multi_data.values)-future_target, past_history,\n",
    "                                                   future_target, STEP)\n",
    "                                                   \n",
    "            x_all = np.append(x_all,x_single,axis=0)\n",
    "            y_all = np.append(y_all,y_single,axis=0)\n",
    "            id_all = np.append(id_all,id*np.ones([y_single.shape[0],1]),axis=0)\n",
    "   \n",
    "        # remove first empty example\n",
    "        x_all = x_all[1:]\n",
    "        y_all = y_all[1:]\n",
    "        id_all = id_all[1:]\n",
    "        x_GV=x_all[:,:,0:1]\n",
    "       \n",
    "        return x_all, y_all, id_all,x_GV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # example of predict() usage\n",
    "\n",
    "    # create Predictor instance\n",
    "    path2data = ''\n",
    "    predictor = Predictor(path2data)\n",
    "\n",
    "    # load the raw data\n",
    "    predictor.load_raw_data(train=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_all = predictor.cgm_and_meals_id_time.reset_index()['id'].unique()\n",
    "# split the data to train and cv - use 80% as train\n",
    "TRAIN_SPLIT = 0.8\n",
    "ids_train = ids_all[range(round(len(ids_all)*TRAIN_SPLIT))]\n",
    "ids_cv = ids_all[:-(round(len(ids_all)*TRAIN_SPLIT))]\n",
    "data_train=predictor.cgm_and_meals_id_time\n",
    "\n",
    "x_train_all, y_train_all, id_train_all,x_train_GV=predictor.build_features(ids=ids_train)\n",
    "x_cv_all, y_cv_all, id_cv_all,x_cv_GV=predictor.build_features(ids=ids_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to file\n",
    "np.savez_compressed('train_all_data_no_normalization', x_train_all=x_train_all, y_train_all=y_train_all,\n",
    "                    id_train_all=id_train_all,x_train_GV=x_train_GV)\n",
    "np.savez_compressed('CV_all_data_no_normalization', x_cv_all=x_cv_all, y_cv_all=y_cv_all,\n",
    "                    id_cv_all=id_cv_all,x_cv_GV=x_cv_GV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size]-target[i-1])\n",
    "\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 12*4\n",
    "future_target = 8\n",
    "STEP = 1\n",
    "\n",
    "# list of id's\n",
    "ids_all = predictor.cgm_and_meals_id_time.reset_index()['id'].unique()\n",
    "\n",
    "# split the data to train and cv - use 80% as train\n",
    "TRAIN_SPLIT = 0.8\n",
    "ids_train = ids_all[range(round(len(ids_all)*TRAIN_SPLIT))]\n",
    "ids_cv = ids_all[:-(round(len(ids_all)*TRAIN_SPLIT))]\n",
    "\n",
    "# ids_train = ids_all[-round(len(ids_all)*0.8):]\n",
    "\n",
    "x_train_all = np.empty([1,past_history,predictor.cgm_and_meals_id_time.shape[1]])\n",
    "y_train_all = np.empty([1,future_target])\n",
    "id_train_all = np.empty([1,1])\n",
    "\n",
    "for id in ids_train:\n",
    "    multi_data = predictor.cgm_and_meals_id_time.loc[id]\n",
    "    end_index = len(multi_data)-future_target\n",
    "    x_train_single, y_train_single = multivariate_data(multi_data.values, multi_data.GlucoseValue.values, 0,\n",
    "                                                   len(multi_data.values)-future_target, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=False)\n",
    "    x_train_all = np.append(x_train_all,x_train_single,axis=0)\n",
    "    y_train_all = np.append(y_train_all,y_train_single,axis=0)\n",
    "    id_train_all = np.append(id_train_all,id*np.ones([y_train_single.shape[0],1]),axis=0)\n",
    "    \n",
    "# remove first empty example\n",
    "x_train_all = x_train_all[1:]\n",
    "y_train_all = y_train_all[1:]\n",
    "id_train_all = id_train_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateIndividuals (self,ids,STEP=1,past_history=48,future_target=8):\n",
    "    \n",
    "    x_all = np.empty([1,past_history,predictor.cgm_and_meals_id_time.shape[1]])\n",
    "    y_all = np.empty([1,future_target])\n",
    "    id_all = np.empty([1,1])\n",
    "\n",
    "    for id in ids_train:\n",
    "        multi_data = predictor.cgm_and_meals_id_time.loc[id]\n",
    "        end_index = len(multi_data)-future_target\n",
    "        x_single, y_single = multivariate_data(multi_data.values, multi_data.GlucoseValue.values, 0,\n",
    "                                                       len(multi_data.values)-future_target, past_history,\n",
    "                                                       future_target, STEP,\n",
    "                                                       single_step=False)\n",
    "        x_train_all = np.append(x_train_all,x_train_single,axis=0)\n",
    "        y_train_all = np.append(y_train_all,y_train_single,axis=0)\n",
    "        id_train_all = np.append(id_train_all,id*np.ones([y_train_single.shape[0],1]),axis=0)\n",
    "\n",
    "    # remove first empty example\n",
    "    x_all = x_all[1:]\n",
    "    y_all = y_all[1:]\n",
    "    id_all = id_all[1:]\n",
    "    \n",
    "    return x_all, y_all, id_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the CV data\n",
    "x_CV_all = np.empty([1,past_history,predictor.cgm_and_meals_id_time.shape[1]])\n",
    "y_CV_all = np.empty([1,future_target])\n",
    "id_CV_all = np.empty([1,1])\n",
    "\n",
    "for id in ids_cv:\n",
    "    multi_cv_data = predictor.cgm_and_meals_id_time.loc[id]\n",
    "    end_index = len(multi_cv_data)-future_target\n",
    "    x_CV_single, y_CV_single = multivariate_data(multi_cv_data.values, multi_cv_data.GlucoseValue.values, 0,\n",
    "                                                   len(multi_cv_data.values)-future_target, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=False)\n",
    "    x_CV_all = np.append(x_CV_all,x_CV_single,axis=0)\n",
    "    y_CV_all = np.append(y_CV_all,y_CV_single,axis=0)\n",
    "    id_CV_all = np.append(id_CV_all,id*np.ones([y_CV_single.shape[0],1]),axis=0)\n",
    "\n",
    "# remove first empty example\n",
    "x_CV_all = x_CV_all[1:]\n",
    "y_CV_all = y_CV_all[1:]\n",
    "id_CV_all = id_CV_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data saved as .npz files\n",
    "#d1 = np.load('train_all_data.npz')\n",
    "#x_train_all_norm = d1['x_train_all']\n",
    "#y_train_all_norm = d1['y_train_all']\n",
    "#id_train_all = d1['id_train_all']\n",
    "\n",
    "#d2 = np.load('CV_all_data.npz')\n",
    "#x_cv_all_norm = d2['x_CV_all']\n",
    "#y_cv_all_norm = d2['y_CV_all']\n",
    "#id_cv_all = d2['id_CV_all']\n",
    "\n",
    "d3 = np.load('train_all_data_no_normalization.npz')\n",
    "x_train_all = d3['x_train_all']\n",
    "y_train_all = d3['y_train_all']\n",
    "id_train_all = d3['id_train_all']\n",
    "x_train_GV = d3['x_train_GV']\n",
    "\n",
    "d4 = np.load('CV_all_data_no_normalization.npz')\n",
    "x_cv_all = d4['x_cv_all']\n",
    "y_cv_all = d4['y_cv_all']\n",
    "id_cv_all = d4['id_cv_all']\n",
    "x_cv_GV = d4['x_cv_GV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_GV = x_train_all[:,:,0:2]\n",
    "# x_train_Meals = x_train_all[:,:,2:]\n",
    "\n",
    "# x_cv_GV = x_cv_all[:,:,0:2]\n",
    "# x_cv_Meals = x_cv_all[:,:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate prediction accuracy using correlation\n",
    "def estimatePredictionUsingCorrelation (x,y,model,y_cv_baseline):\n",
    "\n",
    "    y_predict = model.predict(x)\n",
    "\n",
    "    corr_predition=np.corrcoef(y_predict.flatten(),y.flatten())\n",
    "\n",
    "    import numpy.matlib\n",
    "\n",
    "    y_cv_baseline=np.matlib.repmat(y_cv_baseline,8,1)\n",
    "    y_cv_baseline=np.transpose(y_cv_baseline)\n",
    "\n",
    "    y_cv_mean=np.mean(x[:,:,0],axis=1)\n",
    "    y_cv_mean=np.matlib.repmat(y_cv_mean,8,1)\n",
    "    y_cv_mean=np.transpose(y_cv_mean)\n",
    "\n",
    "    corr_baseline=np.corrcoef(y_cv_baseline.flatten(),y.flatten())\n",
    "    corr_mean=np.corrcoef(y_cv_mean.flatten(),y.flatten())\n",
    "    corr_mean = 0\n",
    "    return corr_predition,corr_baseline, corr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_all, y_train_all))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_cv_all, y_cv_all))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a model with convolutional layers\n",
    "multi_step_model_conv = tf.keras.models.Sequential()\n",
    "multi_step_model_conv.add(tf.keras.layers.Conv1D(3,8, input_shape=x_train_all.shape[-2:])) #,\n",
    "                                         # stateful = 'stateful',\n",
    "                                         # batch_size=BATCH_SIZE))\n",
    "multi_step_model_conv.add(tf.keras.layers.Activation('relu'))\n",
    "multi_step_model_conv.add(tf.keras.layers.Conv1D(6,4))\n",
    "multi_step_model_conv.add(tf.keras.layers.Activation('relu'))\n",
    "multi_step_model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "multi_step_model_conv.add(tf.keras.layers.Flatten())\n",
    "# multi_step_model_conv.add(tf.keras.layers.Activation('relu'))\n",
    "# multi_step_model_conv.add(tf.keras.layers.AveragePooling1D(4))\n",
    "# multi_step_model_conv.add(tf.keras.layers.Conv1D(16,2)) #,\n",
    "# multi_step_model_conv.add(tf.keras.layers.Activation('relu'))\n",
    "# multi_step_model_conv.add(tf.keras.layers.AveragePooling1D(2))\n",
    "multi_step_model_conv.add(tf.keras.layers.Dense(16,activation='relu'))\n",
    "multi_step_model_conv.add(tf.keras.layers.Dense(8)) #,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_model_conv.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 12.3126 - val_loss: 15.9269\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 11.3649 - val_loss: 15.3399\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 11.2900 - val_loss: 14.7320\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 9.9901 - val_loss: 14.2285\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 10.2159 - val_loss: 14.4405\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "EVALUATION_INTERVAL = 200\n",
    "multi_step_history_conv = multi_step_model_conv.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_predition,corr_baseline,corr_mean = estimatePredictionUsingCorrelation (x_cv_all,y_cv_all,multi_step_model_conv,x_cv_all[:,:,1].mean(axis=1)-x_cv_all[:,47,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (correlation): 0.4690251465756874\n",
      "Baseline correlation (last GV measure): 0.4510631920946048\n"
     ]
    }
   ],
   "source": [
    "print('Prediction accuracy (correlation): ' + str(corr_predition[0,1]))\n",
    "print('Baseline correlation (last GV measure): ' + str(corr_baseline[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 12.3093 - val_loss: 15.8837\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 10.1993 - val_loss: 14.0386\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 10.0877 - val_loss: 14.0771\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 9.0913 - val_loss: 13.7967\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 9.3318 - val_loss: 13.9794\n"
     ]
    }
   ],
   "source": [
    "# try the LSTM model using tf DataSet\n",
    "#create the model\n",
    "multi_step_model_LSTM = tf.keras.models.Sequential()\n",
    "multi_step_model_LSTM.add(tf.keras.layers.LSTM(48,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=x_train_all.shape[-2:])) #,\n",
    "                                         # stateful = 'stateful',\n",
    "                                         # batch_size=BATCH_SIZE))\n",
    "multi_step_model_LSTM.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "# multi_step_model.add(tf.keras.layers.Dense(16))\n",
    "multi_step_model_LSTM.add(tf.keras.layers.Dense(8))\n",
    "\n",
    "#multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
    "multi_step_model_LSTM.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# run model\n",
    "multi_step_history_LSTM = multi_step_model_LSTM.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_predition,corr_baseline,corr_mean = estimatePredictionUsingCorrelation (x_cv_all,y_cv_all,multi_step_model_LSTM,x_cv_all[:,:,1].mean(axis=1)-x_cv_all[:,47,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (correlation): 0.5511296488283757\n",
      "Baseline correlation (last GV measure): 0.4510631920946048\n"
     ]
    }
   ],
   "source": [
    "print('Prediction accuracy (correlation): ' + str(corr_predition[0,1]))\n",
    "print('Baseline correlation (last GV measure): ' + str(corr_baseline[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 660808 samples, validate on 164462 samples\n",
      "Epoch 1/5\n",
      " 12800/660808 [..............................] - ETA: 14:19 - loss: 10.8257 - val_loss: 0.0000e+00Epoch 2/5\n",
      " 12800/660808 [..............................] - ETA: 10:07 - loss: 10.9799 - val_loss: 0.0000e+00Epoch 3/5\n",
      " 12800/660808 [..............................] - ETA: 12:33 - loss: 10.7914 - val_loss: 0.0000e+00Epoch 4/5\n",
      " 12800/660808 [..............................] - ETA: 9:56 - loss: 10.8652 - val_loss: 0.0000e+00Epoch 5/5\n",
      " 12800/660808 [..............................] - ETA: 9:53 - loss: 10.6862 - val_loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# try the LSTM model using functional API\n",
    "#create the model\n",
    "input_API = tf.keras.layers.Input(shape=x_train_all.shape[-2:], name='input')\n",
    "\n",
    "# API_cond_first = tf.keras.layers.Conv1D(3,8, activation='relu')(input_API)\n",
    "API_lstm_first = tf.keras.layers.LSTM(48,return_sequences=True)(input_API)\n",
    "API_lstm_second = tf.keras.layers.LSTM(16,return_sequences=True, activation='relu')(API_lstm_first)\n",
    "# API_BN = tf.keras.layers.BatchNormalization()(API_lstm_second)\n",
    "Meals_conv_first = tf.keras.layers.Conv1D(3,8, activation='relu')(API_lstm_second)\n",
    "# Meals_conv_second = tf.keras.layers.Conv1D(6,4, activation='relu')(Meals_conv_first)\n",
    "# Meals_conv_third = tf.keras.layers.Conv1D(8,2, activation='relu')(Meals_conv_second)\n",
    "# Meals_pool = tf.keras.layers.MaxPooling1D(2)(Meals_conv_third)\n",
    "Meals_flat = tf.keras.layers.Flatten()(Meals_conv_first)\n",
    "API_Dense = tf.keras.layers.Dense(8,name='output')(Meals_flat)\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "EVALUATION_INTERVAL = 200\n",
    "\n",
    "API_LSTM_model = tf.keras.Model(inputs=[input_API], outputs=[API_Dense]) \n",
    "API_LSTM_model.compile(optimize='adam',loss='mae')\n",
    "\n",
    "API_LSTM = API_LSTM_model.fit([x_train_all],y_train_all,epochs=EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                              validation_data = [x_cv_all,y_cv_all],\n",
    "                              validation_steps = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_predition,corr_baseline,corr_mean = estimatePredictionUsingCorrelation (x_cv_all,y_cv_all,API_LSTM_model,x_cv_all[:,:,1].mean(axis=1)-x_cv_all[:,47,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (correlation): 0.00017564784582054625\n",
      "Baseline correlation (last GV measure): 0.4510631920946048\n"
     ]
    }
   ],
   "source": [
    "print('Prediction accuracy (correlation): ' + str(corr_predition[0,1]))\n",
    "print('Baseline correlation (last GV measure): ' + str(corr_baseline[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 660808 samples\n",
      "Epoch 1/10\n",
      "128000/660808 [====>.........................] - ETA: 9:03 - loss: 9.5908Epoch 2/10\n",
      "128000/660808 [====>.........................] - ETA: 8:04 - loss: 9.0131Epoch 3/10\n",
      "128000/660808 [====>.........................] - ETA: 8:07 - loss: 8.9417Epoch 4/10\n",
      "128000/660808 [====>.........................] - ETA: 8:11 - loss: 8.9072Epoch 5/10\n",
      "128000/660808 [====>.........................] - ETA: 8:06 - loss: 8.8974Epoch 6/10\n",
      "127944/660808 [====>.........................] - ETA: 8:10 - loss: 8.8845Epoch 7/10\n",
      "128000/660808 [====>.........................] - ETA: 8:04 - loss: 8.8117Epoch 8/10\n",
      "128000/660808 [====>.........................] - ETA: 8:06 - loss: 8.8442Epoch 9/10\n",
      "128000/660808 [====>.........................] - ETA: 8:07 - loss: 8.8531Epoch 10/10\n",
      "128000/660808 [====>.........................] - ETA: 8:00 - loss: 8.8170"
     ]
    }
   ],
   "source": [
    "# try a split model - LSTM for GV, CNN for Meals, FC after concatenation using functional API\n",
    "BATCH_SIZE = 64\n",
    "#create the model\n",
    "# Split net - first branch LSTM + FC for GV, second branch FC for Meals\n",
    "GV_input = tf.keras.layers.Input(shape=x_train_GV.shape[-2:], name='GVinput')\n",
    "Meals_input = tf.keras.layers.Input(shape=x_train_all.shape[-2:], name='Mealsinput')\n",
    "\n",
    "activation = 'relu'\n",
    "\n",
    "# GV branch\n",
    "GV_lstm_first = tf.keras.layers.LSTM(48,return_sequences=True)(GV_input)\n",
    "GV_lstm_second = tf.keras.layers.LSTM(16)(GV_lstm_first)\n",
    "GVoutput = tf.keras.layers.Dense(16,activation='relu',name='GVoutput')(GV_lstm_second)\n",
    "# GVoutput = tf.keras.layers.BatchNormalization()(GV_Dense)\n",
    "\n",
    "# Meals branch\n",
    "Meals_conv1 = tf.keras.layers.Conv1D(6,5, activation='relu')(Meals_input)\n",
    "Meals_conv2 = tf.keras.layers.Conv1D(12,3, activation='relu')(Meals_conv1)\n",
    "Meals_conv3 = tf.keras.layers.Conv1D(24,3, activation='relu')(Meals_conv2)\n",
    "Meals_conv4 = tf.keras.layers.Conv1D(36,3, activation='relu')(Meals_conv3)\n",
    "# Meals_pool = tf.keras.layers.MaxPooling1D(2)(Meals_conv_third)\n",
    "Meals_flat = tf.keras.layers.Flatten()(Meals_conv4)\n",
    "Meals_output = tf.keras.layers.Dense(16,activation=activation,name='Mealsoutput')(Meals_flat)\n",
    "# Meals_output = tf.keras.layers.BatchNormalization()(Meals_conv_second)\n",
    "\n",
    "combined = tf.keras.layers.concatenate([GVoutput, Meals_output])\n",
    "\n",
    "combined_Dense1 = tf.keras.layers.Dense(16,activation=activation)(combined)\n",
    "combined_Dense2 = tf.keras.layers.Dense(16,activation=activation)(combined_Dense1)\n",
    "combined_output = tf.keras.layers.Dense(8,name='combined_output')(combined_Dense2)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "EVALUATION_INTERVAL = 2000\n",
    "\n",
    "combine_model = tf.keras.Model(inputs=[GV_input,Meals_input], outputs=[combined_output]) \n",
    "combine_model.compile(optimize='adam',loss='mae')\n",
    "\n",
    "combine_API_model = combine_model.fit([x_train_GV,x_train_all],y_train_all,epochs=EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,steps_per_epoch=EVALUATION_INTERVAL)#,\n",
    "                              #validation_data = ([x_CV_GV,x_CV_Meals],y_CV_all),\n",
    "                             # validation_steps = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = combine_model.predict([x_cv_GV,x_cv_all])\n",
    "\n",
    "corr_predition=np.corrcoef(y_predict.flatten(),y_cv_all.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (correlation): 0.5861511795894495\n"
     ]
    }
   ],
   "source": [
    "print('Prediction accuracy (correlation): ' + str(corr_predition[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 660808 samples\n",
      "Epoch 1/10\n",
      " 21870/660808 [..............................] - ETA: 8:16 - loss: 10.7275"
     ]
    }
   ],
   "source": [
    "max_batch_size = 500\n",
    "max_num_filters  = [x * 3 for x in [6, 12, 24, 36, 48, 60, 72, 84, 96] ] # do we want increasing size?\n",
    "max_kernel_size = [x * 4 for x in [6, 6, 6, 6, 6, 6, 6]]\n",
    "max_LR = 0.3\n",
    "max_n_LSTM = [x * 4 for x in [48, 16, 16, 16, 16, 16]]\n",
    "max_n_dense = [x * 4 for x in [16, 16, 16, 16, 16]]\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "batch_size = np.empty(1000)\n",
    "EVALUATION_INTERVAL = np.empty(1000)\n",
    "num_filters = np.empty([1000,len(max_num_filters)])\n",
    "kernel_size = np.empty([1000,len(max_kernel_size)])\n",
    "LR = np.empty(1000)\n",
    "n_LSTM = np.empty([1000,len(max_n_LSTM)])\n",
    "n_dense = np.empty([1000,len(max_n_dense)])\n",
    "\n",
    "\n",
    "for ii_ite in list(range(1,1000)):\n",
    "   \n",
    "    tmp_batch_size = int(round((max_batch_size*np.random.rand())))\n",
    "    batch_size[ii_ite] = tmp_batch_size\n",
    "    \n",
    "    #EVALUATION_INTERVAL = (max_EVALUATION_INTERVAL*rand_nums[1]).round()\n",
    "    tmp_EVALUATION_INTERVAL=(660000/(batch_size[ii_ite]*5)).round().astype(int)\n",
    "    EVALUATION_INTERVAL[ii_ite] = tmp_EVALUATION_INTERVAL\n",
    "    \n",
    "    EPOCHS = (EPOCHS*batch_size[ii_ite]/100).round().astype(int)\n",
    "    if EPOCHS<10:\n",
    "        EPOCHS=10\n",
    "        \n",
    "    tmp_num_filters = (max_num_filters*np.random.rand(len(max_num_filters))).round().astype(int)\n",
    "    num_filters[ii_ite,:] = tmp_num_filters\n",
    "    \n",
    "    tmp_kernel_size = (max_kernel_size*np.random.rand(len(max_kernel_size))).round().astype(int).tolist()\n",
    "    kernel_size[ii_ite,:] = tmp_kernel_size\n",
    "    \n",
    "    LR[ii_ite] = max_LR*np.random.rand(1)\n",
    "    \n",
    "    tmp_n_LSTM = (max_n_LSTM*np.random.rand(len(max_n_LSTM))).round().astype(int)\n",
    "    n_LSTM[ii_ite,:] = tmp_n_LSTM\n",
    "\n",
    "    tmp_n_dense = (max_n_dense*np.random.rand(len(max_n_dense))).round().astype(int)\n",
    "    n_dense[ii_ite,:] = tmp_n_dense\n",
    "   \n",
    "   \n",
    "    #train (make sure to save loss):\n",
    "    predictor.define_nn_hypPars(BATCH_SIZE = tmp_batch_size, EVALUATION_INTERVAL = tmp_EVALUATION_INTERVAL, \n",
    "                                EPOCHS = EPOCHS, num_filters = tmp_num_filters,kernel_size = tmp_kernel_size, \n",
    "                                LR = LR[ii_ite], n_LSTM = tmp_n_LSTM, n_dense = tmp_n_dense)\n",
    "    predictor.define_nn(GV_shape=x_train_GV.shape[-2:],MealsAndGV_shape=x_train_all.shape[-2:])\n",
    "    \n",
    "    predictor.train_nn(X_train=x_train_all,y_train=y_train_all)\n",
    "    del predictor.nn\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (correlation): 0.6503797284441221\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "predictor.nn = tf.keras.models.load_model(predictor.path2data+'123-337-49.hdf5')\n",
    "\n",
    "y_predict = predictor.nn.predict([x_cv_GV,x_cv_all])\n",
    "\n",
    "corr_predition=np.corrcoef(y_predict.flatten(),y_cv_all.flatten())\n",
    "print('Prediction accuracy (correlation): ' + str(corr_predition[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Predictor' object has no attribute 'nn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0dbea9d38e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Predictor' object has no attribute 'nn'"
     ],
     "output_type": "error"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
